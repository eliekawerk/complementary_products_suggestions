{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models pipeline\n",
    "## Siamese CNN and Siamese LSTM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from complementary_products_suggestions import helper_functions, embeddings, config\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "import scikitplot as skplt\n",
    "from sklearn.metrics import classification_report\n",
    "import os\n",
    "import datetime\n",
    "import tensorflow.python as tf\n",
    "from tensorflow.python.keras.layers import Input, LSTM, dot, Embedding, Conv1D, Flatten, Dense, Dropout, Activation, MaxPooling1D, ZeroPadding1D\n",
    "from tensorflow.python.keras.models import Sequential, Model\n",
    "from tensorflow.python.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard\n",
    "from tensorflow.python.keras import backend as K\n",
    "from tensorflow.python.keras.regularizers import l1, l2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieving the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "database = pd.read_csv(\"../dummy_sample_matches.csv\")\n",
    "content = pd.read_csv(\"../dummy_sample_content.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting the data in train-test split\n",
    "We are using GroupShuffleSplit as we want to make sure that the products that appear as add-ons in the train set will not appear as an add-on in the test set. We do this to make sure that the model performance will be evaluated on unseen data (real-life scenarios)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = helper_functions.train_test_split(database, 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Word2vec to create embeddings for each word in product titles based on the whole corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_weights = embeddings.word2vec(content, X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t, train_set_main, train_set_addon, test_set_main, test_set_addon = helper_functions.tokenize_train_test_set(X_train, X_test, 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Siamese CNN\n",
    "The difference for the Late Merge (LM) or Intermediate Merge (IM) is accordingly documented in the code below. Uncomment the commented lines to get the configuration as described."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input1_layer = Input(shape=(train_set_main.shape[1],))\n",
    "input2_layer = Input(shape=(train_set_addon.shape[1],))\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "#Embedding layer with pre-initialized weights from word2vec\n",
    "model.add(Embedding(input_dim=len(t.word_index)+1, \n",
    "                    output_dim=config.feature_dim,\n",
    "                    weights=[embedding_weights],\n",
    "                    input_length=30,\n",
    "                    trainable=False))\n",
    "\n",
    "model.add(ZeroPadding1D(padding=(config.filter1_length-1)))\n",
    "model.add(Conv1D(filters=config.nb_filter,\n",
    "                kernel_size=config.filter1_length,\n",
    "                padding=config.padding,\n",
    "                activation=config.activation,\n",
    "                kernel_regularizer=l2(config.regularizer),\n",
    "                bias_regularizer=l2(config.regularizer),\n",
    "                activity_regularizer=l2(config.regularizer)))\n",
    "\n",
    "model.add(MaxPooling1D(pool_size=config.pool1_length))\n",
    "\n",
    "model.add(ZeroPadding1D(padding=config.filter2_length-1))\n",
    "model.add(Conv1D(filters=config.nb_filter,\n",
    "                kernel_size=config.filter2_length,\n",
    "                padding=config.padding,\n",
    "                activation=config.activation,\n",
    "                kernel_regularizer=l2(config.regularizer),\n",
    "                bias_regularizer=l2(config.regularizer),\n",
    "                activity_regularizer=l2(config.regularizer)))\n",
    "\n",
    "model.add(MaxPooling1D(pool_size=config.pool2_length))\n",
    "\n",
    "model.add(Dropout(config.dropout_rate))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "#Intermediate merge start\n",
    "# encoded_main = model(input1_layer)\n",
    "# encoded_addon = model(input2_layer)\n",
    "\n",
    "# merged_layer = dot([encoded_main, encoded_addon], axes=1, trainable=True)\n",
    "\n",
    "# dense = Dense(config.nb_neurons_dense, \n",
    "#                 activation=config.activation)(merged_layer)\n",
    "\n",
    "# # Add a dense layer with a sigmoid unit to generate the similarity score\n",
    "# prediction = Dense(1,\n",
    "#                    activation='sigmoid')(dense)\n",
    "#Intermediate merge end\n",
    "\n",
    "\n",
    "#Late merge start\n",
    "model.add(Dense(config.nb_neurons_dense, \n",
    "                activation=config.activation))\n",
    "\n",
    "encoded_main = model(input1_layer)\n",
    "encoded_addon = model(input2_layer)\n",
    "merged_layer = dot([encoded_main, encoded_addon],\n",
    "                   axes=1,\n",
    "                   trainable=True)\n",
    "\n",
    "# Add a dense layer with a sigmoid unit to generate the similarity score\n",
    "prediction = Dense(1,\n",
    "                   activation='sigmoid')(merged_layer)\n",
    "#Late merge end\n",
    "\n",
    "\n",
    "# Connect the inputs with the outputs\n",
    "siamese_cnn = Model(inputs=[input1_layer,input2_layer],\n",
    "                    outputs=prediction)\n",
    "\n",
    "siamese_cnn.compile(optimizer=config.optimizer,\n",
    "                    loss='binary_crossentropy', \n",
    "                    metrics=['accuracy'])\n",
    "\n",
    "logdir = os.path.join(\"logs-cnn\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "tensorboard = TensorBoard(logdir, histogram_freq=1)\n",
    "callbacks = [EarlyStopping(monitor='val_loss', patience=config.stop_epochs, verbose=1, mode='auto'), tensorboard]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the Siamese CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_cnn = siamese_cnn.fit([train_set_main, train_set_addon],\n",
    "                              y_train,\n",
    "                              validation_split=0.1,\n",
    "                              batch_size=config.batch_size,\n",
    "                              epochs=config.nb_epochs,\n",
    "                              callbacks=callbacks,\n",
    "                              verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making predictions for the test set using the Siamese CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_cnn = siamese_cnn.predict([test_set_main, test_set_addon],\n",
    "                                 verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combining the predicted values (scores) with the real values for the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_cnn = X_test\n",
    "X_test_cnn['predicted_label'] = pd.Series(np.round(y_pred_cnn.ravel(),3), index=X_test_cnn.index)\n",
    "X_test_cnn['real_label'] = pd.Series(y_test, index=X_test_cnn.index)\n",
    "X_test_cnn.tail(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Siamese LSTM\n",
    "The difference for the Late Merge (LM) or Intermediate Merge (IM) is accordingly documented in the code below. Uncomment the commented lines to get the configuration as described."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_1 = Input(shape=(train_set_main.shape[1],))\n",
    "input_2 = Input(shape=(train_set_addon.shape[1],))\n",
    "\n",
    "common_embed = Embedding(input_dim =len(t.word_index)+1,\n",
    "                         weights=[embedding_weights],\n",
    "                         trainable=False,\n",
    "                         output_dim=config.feature_dim,\n",
    "                         input_length=30) \n",
    "\n",
    "lstm_1 = common_embed(input_1)\n",
    "lstm_2 = common_embed(input_2)\n",
    "\n",
    "common_lstm = LSTM(config.nb_neurons_lstm,\n",
    "                   return_sequences=True, \n",
    "                   activation=config.activation,\n",
    "                   kernel_regularizer=l2(config.regularizer),\n",
    "                   bias_regularizer=l2(config.regularizer),\n",
    "                   activity_regularizer=l2(config.regularizer))\n",
    "\n",
    "vector_1 = common_lstm(lstm_1)\n",
    "vector_1 = Flatten(name='flatten1')(vector_1)\n",
    "\n",
    "vector_2 = common_lstm(lstm_2)\n",
    "vector_2 = Flatten(name='flatten2')(vector_2)\n",
    "\n",
    "#Intermediate merge start\n",
    "conc = dot([vector_1, vector_2],\n",
    "           axes=1,\n",
    "           normalize=True,\n",
    "           name='dot')\n",
    "\n",
    "x = Dense(config.nb_neurons_dense,\n",
    "          activation=config.activation,\n",
    "          name='conc_layer')(conc)\n",
    "\n",
    "x = Dropout(config.dropout_rate)(x)\n",
    "#Intermediate merge end\n",
    "\n",
    "#Late merge start\n",
    "# x_1 = Dense(config.np_neurons_dense,\n",
    "#             activation=config.activation,\n",
    "#             name='conc_layer')(vector_1)\n",
    "\n",
    "# x_2 = Dense(config.np_neurons_dense,\n",
    "#             activation=config.activation,\n",
    "#             name='conc_layer')(vector_2)\n",
    "\n",
    "# x_1 = Dropout(config.dropout_rate)(x_1)\n",
    "# x_2 = Dropout(config.dropout_rate)(x_2)\n",
    "\n",
    "# conc = dot([x_1, x_2],\n",
    "#            axes=1,\n",
    "#            normalize=True,\n",
    "#            name='dot')\n",
    "#Late merge end\n",
    "\n",
    "out = Dense(1,\n",
    "            activation=\"sigmoid\",\n",
    "            name = 'out')(x)\n",
    "\n",
    "siamese_lstm = Model([input_1, input_2],\n",
    "                     out)\n",
    "\n",
    "siamese_lstm.compile(loss='binary_crossentropy',\n",
    "                     optimizer=config.optimizer,\n",
    "                     metrics=['accuracy'])\n",
    "\n",
    "logdir = os.path.join(\"logs-lstm\",\n",
    "                      datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "tensorboard = TensorBoard(logdir, histogram_freq=1)\n",
    "callbacks = [EarlyStopping(monitor='val_loss',patience=config.stop_epochs, verbose=1, mode='auto'),\n",
    "             tensorboard]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the Siamese LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_lstm = siamese_lstm.fit([train_set_main, train_set_addon],\n",
    "                                y_train,\n",
    "                                validation_split=0.1,\n",
    "                                batch_size=config.batch_size,\n",
    "                                epochs=config.nb_epochs,\n",
    "                                callbacks=callbacks,\n",
    "                                verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the Siamese LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_lstm = siamese_lstm.predict([test_set_main, test_set_addon],\n",
    "                                    verbose=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combining the predictons scores for the test set with the real values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_lstm = X_test\n",
    "X_test_lstm['predicted_label'] = pd.Series(np.round(y_pred_lstm.ravel(),3), index=X_test_lstm.index)\n",
    "X_test_lstm['real_label'] = pd.Series(y_test, index=X_test_lstm.index)\n",
    "X_test_lstm.tail(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing the results\n",
    "All result graphs and metrics can be used for both siamese_cnn and siamese_lstm networks. We just need to change the name when we want to show specific outcomes for one of them. Where we do comparative analysis, we keep both model outputs in the graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC - AUC curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for siamese_cnn\n",
    "y_probas_cnn = np.concatenate((1-y_pred_cnn,y_pred_cnn),axis=1)\n",
    "\n",
    "fig = plt.figure()\n",
    "skplt.metrics.plot_roc_curve(y_test, y_probas_cnn)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for siamese_lstm\n",
    "y_probas_lstm = np.concatenate((1-y_pred_lstm,y_pred_lstm),axis=1)\n",
    "\n",
    "fig = plt.figure()\n",
    "skplt.metrics.plot_roc_curve(y_test, y_probas_lstm)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results and analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for siamese_cnn\n",
    "auc = sklearn.metrics.roc_auc_score(y_test, y_pred_cnn)*100\n",
    "acc = sklearn.metrics.accuracy_score(y_test, y_pred_cnn.ravel() > 0.5)*100\n",
    "print('AUC for Siamese CNN: %s\\n' % auc)\n",
    "print('Accuracy for Siamese CNN: %s\\n' % acc)\n",
    "\n",
    "print(sklearn.metrics.confusion_matrix(y_test, y_pred_cnn.ravel() > 0.5))\n",
    "print(classification_report(y_test, y_pred_cnn.ravel() > 0.5))\n",
    "\n",
    "plt.figure(figsize=(10,10));\n",
    "plt.hist(y_pred_cnn[y_test == 0], bins=50, color='red', alpha=0.7);\n",
    "plt.hist(y_pred_cnn[y_test == 1], bins=50, color='green', alpha=0.7);\n",
    "plt.text(0.4, 1000, \"Siamese CNN\", fontsize=18)\n",
    "plt.xlabel(\"probability score\")\n",
    "plt.ylabel(\"samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for siamese_lstm\n",
    "auc = sklearn.metrics.roc_auc_score(y_test, y_pred_lstm)*100\n",
    "acc = sklearn.metrics.accuracy_score(y_test, y_pred_lstm.ravel() > 0.5)*100\n",
    "print('AUC for Siamese LSTM: %s\\n' % auc)\n",
    "print('Accuracy for Siamese LSTM: %s\\n' % acc)\n",
    "\n",
    "print(sklearn.metrics.confusion_matrix(y_test, y_pred_lstm.ravel() > 0.5))\n",
    "print(classification_report(y_test, y_pred_lstm.ravel() > 0.5))\n",
    "\n",
    "plt.figure(figsize=(10,10));\n",
    "plt.hist(y_pred_lstm[y_test == 0], bins=50, color='red', alpha=0.7);\n",
    "plt.hist(y_pred_lstm[y_test == 1], bins=50, color='green', alpha=0.7);\n",
    "plt.text(0.7, 3900, \"Siamese LSTM\", fontsize=18)\n",
    "plt.xlabel(\"probability score\")\n",
    "plt.ylabel(\"samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot accuracy and loss curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list all data in history\n",
    "print(history_cnn.history.keys())\n",
    "print(\"Siamese CNN\")\n",
    "\n",
    "# summarize history for accuracy\n",
    "plt.plot(history_cnn.history['accuracy'])\n",
    "plt.plot(history_cnn.history['val_accuracy'])\n",
    "plt.title('Siamese CNN accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# summarize history for loss\n",
    "plt.plot(history_cnn.history['loss'])\n",
    "plt.plot(history_cnn.history['val_loss'])\n",
    "plt.title('Siamese CNN loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list all data in history\n",
    "print(history_lstm.history.keys())\n",
    "print(\"Siamese LSTM\")\n",
    "\n",
    "# summarize history for accuracy\n",
    "plt.plot(history_lstm.history['accuracy'])\n",
    "plt.plot(history_lstm.history['val_accuracy'])\n",
    "plt.title('Siamese LSTM accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# summarize history for loss\n",
    "plt.plot(history_lstm.history['loss'])\n",
    "plt.plot(history_lstm.history['val_loss'])\n",
    "plt.title('Siamese LSTM loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
