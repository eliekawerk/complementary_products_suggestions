{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the value of adding word2vec embeddings before the Embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from complementary_products_suggestions import helper_functions, embeddings, config\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "import scikitplot as skplt\n",
    "from sklearn.metrics import classification_report\n",
    "import os\n",
    "import datetime\n",
    "import timeit\n",
    "import tensorflow.python as tf\n",
    "from tensorflow.keras.layers import Input, LSTM, dot, Embedding, Conv1D, Flatten, Dense, Dropout, Activation, MaxPooling1D, ZeroPadding1D\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.regularizers import l1, l2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retreiving the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "database = pd.read_pickle(\"../dummy_sample_matches.csv\")\n",
    "content = pd.read_pickle(\"../dummy_sample_content.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting the data in train-test split\n",
    "We are using GroupShuffleSplit as we want to make sure that the products that appear as add-ons in the train set will not appear as an add-on in the test set. We do this to make sure that the model performance will be evaluated on unseen data (real-life scenarios)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = helper_functions.train_test_split(database, 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Word2vec to create embeddings for each word in product titles based on the whole corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_weights = embeddings.word2vec(content, X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t, train_set_main, train_set_addon, test_set_main, test_set_addon = helper_functions.tokenize_train_test_set(X_train, X_test, 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Siamese LSTM with pretrained embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_1 = Input(shape=(train_set_main.shape[1],))\n",
    "input_2 = Input(shape=(train_set_addon.shape[1],))\n",
    "\n",
    "common_embed = Embedding(input_dim =len(t.word_index)+1,\n",
    "                         weights=[embedding_weights],\n",
    "                         trainable=True,\n",
    "                         output_dim=config.feature_dim,\n",
    "                         input_length=30) \n",
    "\n",
    "lstm_1 = common_embed(input_1)\n",
    "lstm_2 = common_embed(input_2)\n",
    "\n",
    "common_lstm = LSTM(config.nb_neurons_lstm,\n",
    "                   return_sequences=True, \n",
    "                   activation=config.activation,\n",
    "                   kernel_regularizer=l2(config.regularizer),\n",
    "                   bias_regularizer=l2(config.regularizer),\n",
    "                   activity_regularizer=l2(config.regularizer))\n",
    "\n",
    "vector_1 = common_lstm(lstm_1)\n",
    "vector_1 = Flatten(name='flatten1')(vector_1)\n",
    "\n",
    "vector_2 = common_lstm(lstm_2)\n",
    "vector_2 = Flatten(name='flatten2')(vector_2)\n",
    "\n",
    "conc = dot([vector_1, vector_2],\n",
    "           axes=1,\n",
    "           normalize=True,\n",
    "           name='dot')\n",
    "\n",
    "x = Dense(config.nb_neurons_dense,\n",
    "          activation=config.activation,\n",
    "          name='conc_layer')(conc)\n",
    "\n",
    "x = Dropout(config.dropout_rate)(x)\n",
    "\n",
    "out = Dense(1,\n",
    "            activation=\"sigmoid\",\n",
    "            name = 'out')(x)\n",
    "\n",
    "siamese_lstm_with_word2vec = Model([input_1, input_2],\n",
    "                                   out)\n",
    "\n",
    "siamese_lstm_with_word2vec.compile(loss='binary_crossentropy',\n",
    "                                   optimizer=config.optimizer,\n",
    "                                   metrics=['accuracy'])\n",
    "\n",
    "logdir = os.path.join(\"logs-lstm\",\n",
    "                      datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "tensorboard = TensorBoard(logdir, histogram_freq=1)\n",
    "callbacks = [EarlyStopping(monitor='val_loss',patience=config.stop_epochs, verbose=1, mode='auto'),\n",
    "             tensorboard]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = timeit.default_timer()\n",
    "history_lstm_with_word2vec = siamese_lstm_with_word2vec.fit([train_set_main, train_set_addon],\n",
    "                                                            y_train,\n",
    "                                                            validation_split=0.1,\n",
    "                                                            batch_size=config.batch_size,\n",
    "                                                            epochs=config.nb_epochs,\n",
    "                                                            callbacks=callbacks,\n",
    "                                                            verbose=1)\n",
    "stop = timeit.default_timer()\n",
    "print(f\"Time: {stop-start}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_lstm_with_word2vec = siamese_lstm_with_word2vec.predict([test_set_main, test_set_addon],\n",
    "                                                               verbose=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combining the predictons scores for the test set with the real values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_lstm_with_word2vec = X_test\n",
    "X_test_lstm_with_word2vec['predicted_label'] = pd.Series(np.round(y_pred_lstm_with_word2vec.ravel(),3), index=X_test_lstm_with_word2vec.index)\n",
    "X_test_lstm_with_word2vec['real_label'] = pd.Series(y_test, index=X_test_lstm_with_word2vec.index)\n",
    "X_test_lstm_with_word2vec.tail(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Siamese LSTM without pretrained word embeddings \n",
    "The only difference is in the Embedding layer under the parameters *weights* and *trainable* (we simply remove them)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_1 = Input(shape=(train_set_main.shape[1],))\n",
    "input_2 = Input(shape=(train_set_addon.shape[1],))\n",
    "\n",
    "common_embed = Embedding(input_dim =len(t.word_index)+1,\n",
    "                         output_dim=config.feature_dim,\n",
    "                         input_length=30) \n",
    "\n",
    "lstm_1 = common_embed(input_1)\n",
    "lstm_2 = common_embed(input_2)\n",
    "\n",
    "common_lstm = LSTM(config.nb_neurons_lstm,\n",
    "                   return_sequences=True, \n",
    "                   activation=config.activation,\n",
    "                   kernel_regularizer=l2(config.regularizer),\n",
    "                   bias_regularizer=l2(config.regularizer),\n",
    "                   activity_regularizer=l2(config.regularizer))\n",
    "\n",
    "vector_1 = common_lstm(lstm_1)\n",
    "vector_1 = Flatten(name='flatten1')(vector_1)\n",
    "\n",
    "vector_2 = common_lstm(lstm_2)\n",
    "vector_2 = Flatten(name='flatten2')(vector_2)\n",
    "\n",
    "conc = dot([vector_1, vector_2],\n",
    "           axes=1,\n",
    "           normalize=True,\n",
    "           name='dot')\n",
    "\n",
    "x = Dense(config.nb_neurons_dense,\n",
    "          activation=config.activation,\n",
    "          name='conc_layer')(conc)\n",
    "\n",
    "x = Dropout(config.dropout_rate)(x)\n",
    "\n",
    "out = Dense(1,\n",
    "            activation=\"sigmoid\",\n",
    "            name = 'out')(x)\n",
    "\n",
    "\n",
    "siamese_lstm_without_word2vec = Model([input_1, input_2],\n",
    "                     out)\n",
    "\n",
    "siamese_lstm_without_word2vec.compile(loss='binary_crossentropy',\n",
    "                                      optimizer=config.optimizer,\n",
    "                                      metrics=['accuracy'])\n",
    "\n",
    "logdir = os.path.join(\"logs-lstm\",\n",
    "                      datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "tensorboard = TensorBoard(logdir, histogram_freq=1)\n",
    "callbacks = [EarlyStopping(monitor='val_loss',patience=config.stop_epochs, verbose=1, mode='auto'),\n",
    "             tensorboard]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = timeit.default_timer()\n",
    "history_lstm_without_word2vec = siamese_lstm_without_word2vec.fit([train_set_main, train_set_addon],\n",
    "                                                                  y_train,\n",
    "                                                                  validation_split=0.1,\n",
    "                                                                  batch_size=config.batch_size,\n",
    "                                                                  epochs=config.nb_epochs,\n",
    "                                                                  callbacks=callbacks,\n",
    "                                                                  verbose=1)\n",
    "stop = timeit.default_timer()\n",
    "print(f\"Time: {stop-start}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_lstm_without_word2vec = siamese_lstm_without_word2vec.predict([test_set_main, test_set_addon],\n",
    "                                                                     verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_lstm_without_word2vec = X_test\n",
    "X_test_lstm_without_word2vec['predicted_label'] = pd.Series(np.round(y_pred_lstm_without_word2vec.ravel(),3), index=X_test_lstm_without_word2vec.index)\n",
    "X_test_lstm_without_word2vec['real_label'] = pd.Series(y_test, index=X_test_lstm_without_word2vec.index)\n",
    "X_test_lstm_without_word2vec.tail(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparative Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list all data in history\n",
    "print(history_lstm_with_word2vec.history.keys())\n",
    "print(history_lstm_without_word2vec.history.keys())\n",
    "\n",
    "# summarize history for accuracy\n",
    "plt.plot(history_lstm_with_word2vec.history['val_accuracy'])\n",
    "plt.plot(history_lstm_without_word2vec.history['val_accuracy'])\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['with Word2vec', 'without Word2vec'], loc='lower right')\n",
    "plt.show()\n",
    "\n",
    "# summarize history for loss\n",
    "plt.plot(history_lstm_with_word2vec.history['val_loss'])\n",
    "plt.plot(history_lstm_without_word2vec.history['val_loss'])\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['with Word2vec', 'without Word2vec'],  loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for siamese lstm with word2vec\n",
    "auc = sklearn.metrics.roc_auc_score(y_test, y_pred_lstm_with_word2vec)*100\n",
    "acc = sklearn.metrics.accuracy_score(y_test, y_pred_lstm_with_word2vec.ravel() > 0.5)*100\n",
    "print('AUC for Siamese LSTM with word2vec %s\\n' % auc)\n",
    "print('Accuracy for Siamese LSTM with word2vec: %s\\n' % acc)\n",
    "\n",
    "print(sklearn.metrics.confusion_matrix(y_test, y_pred_lstm_with_word2vec.ravel() > 0.5))\n",
    "print(classification_report(y_test, y_pred_lstm_with_word2vec.ravel() > 0.5))\n",
    "\n",
    "plt.figure(figsize=(10,10));\n",
    "plt.hist(y_pred_lstm_with_word2vec[y_test == 0], bins=50, color='red', alpha=0.7);\n",
    "plt.hist(y_pred_lstm_with_word2vec[y_test == 1], bins=50, color='green', alpha=0.7);\n",
    "plt.text(0.2, 5000, \"Siamese LSTM with word2vec\", fontsize=18)\n",
    "plt.xlabel(\"probability score\")\n",
    "plt.ylabel(\"samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for siamese lstm without word2vec\n",
    "auc = sklearn.metrics.roc_auc_score(y_test, y_pred_lstm_without_word2vec)*100\n",
    "acc = sklearn.metrics.accuracy_score(y_test, y_pred_lstm_without_word2vec.ravel() > 0.5)*100\n",
    "print('AUC for Siamese LSTM without word2vec %s\\n' % auc)\n",
    "print('Accuracy for Siamese LSTM without word2vec: %s\\n' % acc)\n",
    "print(sklearn.metrics.confusion_matrix(y_test, y_pred_lstm_without_word2vec.ravel() > 0.5))\n",
    "print(classification_report(y_test, y_pred_lstm_without_word2vec.ravel() > 0.5))\n",
    "\n",
    "plt.figure(figsize=(10,10));\n",
    "plt.hist(y_pred_lstm_without_word2vec[y_test == 0], bins=50, color='red', alpha=0.7);\n",
    "plt.hist(y_pred_lstm_without_word2vec[y_test == 1], bins=50, color='green', alpha=0.7);\n",
    "plt.text(0.2, 3900, \"Siamese LSTM without word2vec\", fontsize=18)\n",
    "plt.xlabel(\"probability score\")\n",
    "plt.ylabel(\"samples\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
