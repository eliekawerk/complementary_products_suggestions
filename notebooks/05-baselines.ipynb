{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from complementary_products_suggestions import helper_functions, embeddings, config, data_preprocessing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "import scikitplot as skplt\n",
    "from sklearn.metrics import classification_report\n",
    "import os\n",
    "import datetime\n",
    "import tensorflow.python as tf\n",
    "from tensorflow.keras.layers import Input, LSTM, dot, Embedding, Conv1D, Flatten, Dense, Dropout, Activation, MaxPooling1D, ZeroPadding1D\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.regularizers import l1, l2\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retreiving the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "database = pd.read_pickle(\"../dummy_sample_matches.csv\")\n",
    "content = pd.read_pickle(\"../dummy_sample_content.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "database['combined'] = database[['title_main', 'title_addon']].apply(lambda x: ' '.join(x), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting the data in train-test split\n",
    "We are using GroupShuffleSplit as we want to make sure that the products that appear as add-ons in the train set will not appear as an add-on in the test set. We do this to make sure that the model performance will be evaluated on unseen data (real-life scenarios)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = helper_functions.train_test_split(database, 0.2, single=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper function which is repeating in this notebook for every classifer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def results(y_pred):\n",
    "    auc = sklearn.metrics.roc_auc_score(y_test, y_pred)*100\n",
    "    acc = sklearn.metrics.accuracy_score(y_test, y_pred.ravel() > 0.5)*100\n",
    "    print('AUC: %s\\n' % auc)\n",
    "    print('Accuracy: %s\\n' % acc)\n",
    "    print(sklearn.metrics.confusion_matrix(y_test, y_pred.ravel() > 0.5))\n",
    "#     y_pred_bool = np.argmax(y_pred, axis=1)\n",
    "    print(classification_report(y_test, y_pred.ravel() > 0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are using Count Vectorizer for transforming the data before the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_text = pd.concat([X_train['combined'], X_test['combined']])\n",
    "word_vectorizer = CountVectorizer(analyzer='word', lowercase=True)\n",
    "word_vectorizer.fit(X_train['combined'])\n",
    "train_features = word_vectorizer.transform(X_train['combined'])\n",
    "test_features = word_vectorizer.transform(X_test['combined'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(n_estimators = 100, random_state = 42, verbose=1)\n",
    "rf.fit(train_features, y_train)\n",
    "y_pred_rf = rf.predict(test_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reporting the results using the previously defined function for analyzing the performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results(y_pred_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2vec and tokenization for the single neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Word2vec to create embeddings for each word in product titles based on the whole corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_weights = embeddings.word2vec(content, X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing the data\n",
    "We tokenize the combined data (main + add-on product) as we are now dealing with single neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t, train_set_combined, test_set_combined = helper_functions.tokenize_train_test_set(X_train, X_test, 60, single=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vanilla NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_input = Input(shape=(train_set_combined.shape[1],))\n",
    "\n",
    "embedding_layer = Embedding(input_dim =len(t.word_index)+1,\n",
    "                            weights=[embedding_weights],\n",
    "                            output_dim=config.feature_dim,\n",
    "                            input_length=60,\n",
    "                            trainable=False) \n",
    "\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "\n",
    "flatten = Flatten()(embedded_sequences)\n",
    "x = Dense(config.nb_neurons_dense, activation=config.activation)(flatten)\n",
    "\n",
    "x = Dropout(config.dropout_rate)(x)\n",
    "\n",
    "out = Dense(1, activation=\"sigmoid\", name = 'out')(x)\n",
    "\n",
    "vanilla_nn = Model(sequence_input,\n",
    "                   out)\n",
    "\n",
    "vanilla_nn.compile(loss='binary_crossentropy',\n",
    "                   optimizer=config.optimizer,\n",
    "                   metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_vanila_nn = vanilla_nn.fit(train_set_combined,\n",
    "                                   y_train,\n",
    "                                   validation_split=0.2,\n",
    "                                   batch_size=config.batch_size,\n",
    "                                   epochs=config.nb_epochs,\n",
    "                                   verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_vanilla_nn = vanilla_nn.predict(test_set_combined,\n",
    "                                       verbose=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results from the Vanilla NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results(y_pred_vanilla_nn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_1 = Input(shape=(train_set_combined.shape[1],))\n",
    "\n",
    "common_embed = Embedding(input_dim =len(t.word_index)+1,\n",
    "                         weights=[embedding_weights],\n",
    "                         output_dim=config.feature_dim,\n",
    "                         input_length=60,\n",
    "                         trainable=False) \n",
    "\n",
    "lstm_1 = common_embed(input_1)\n",
    "\n",
    "common_lstm = LSTM(150,\n",
    "                   return_sequences=True, \n",
    "                   activation=\"relu\",\n",
    "                   kernel_regularizer=l2(config.regularizer),\n",
    "                   bias_regularizer=l2(config.regularizer),\n",
    "                   activity_regularizer=l2(config.regularizer))\n",
    "\n",
    "vector_1 = common_lstm(lstm_1)\n",
    "vector_1 = Flatten(name='flatten1')(vector_1)\n",
    "\n",
    "x = Dense(config.nb_neurons_dense, activation=config.activation, name='conc_layer')(vector_1)\n",
    "\n",
    "x = Dropout(0.01)(x)\n",
    "\n",
    "out = Dense(1, activation=\"sigmoid\", name = 'out')(x)\n",
    "\n",
    "single_lstm = Model(input_1, out)\n",
    "\n",
    "single_lstm.compile(loss='binary_crossentropy',\n",
    "                    optimizer=config.optimizer,\n",
    "                    metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_single_lstm = single_lstm.fit(train_set_combined,\n",
    "                                      y_train,\n",
    "                                      validation_split=0.1,\n",
    "                                      batch_size=config.batch_size,\n",
    "                                      epochs=config.nb_epochs,\n",
    "                                      verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_single_lstm = single_lstm.predict(test_set_combined,\n",
    "                                         verbose=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reporting the results from the single LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results(y_pred_single_lstm)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
